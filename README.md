# Traffic Light Optimization using Deep Q-Learning

An intelligent traffic signal control system that uses Deep Reinforcement Learning to optimize traffic flow at intersections. The AI agent learns to minimize vehicle waiting times through trial and error, adapting to different traffic patterns.

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow 2.20](https://img.shields.io/badge/tensorflow-2.20-orange.svg)](https://www.tensorflow.org/)
[![SUMO 1.24](https://img.shields.io/badge/SUMO-1.24-green.svg)](https://www.eclipse.org/sumo/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ¯ Overview

Traditional traffic lights operate on fixed timers, causing unnecessary delays and congestion. This project implements an adaptive traffic control system using:

- **Deep Q-Learning**: Reinforcement learning algorithm for decision making
- **Neural Networks**: 5-layer deep network with 400 neurons per layer
- **SUMO Simulator**: Realistic traffic simulation environment
- **Experience Replay**: Stable learning from past experiences

### Key Features

- âœ… **Real-time Decision Making**: AI observes traffic and adapts signal timings
- âœ… **Learning from Experience**: Improves over 100+ training episodes
- âœ… **Smart Traffic Management**: Minimizes cumulative waiting time
- âœ… **Visualization**: Training metrics and testing results with graphs

---

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.12+** - [Download](https://www.python.org/downloads/)
2. **SUMO 1.24+** - [Download](https://www.eclipse.org/sumo/)
3. **Git** - [Download](https://git-scm.com/)

### Installation

```bash
# Clone the repository
git clone https://github.com/sumitsingh3072/Traffic-Light-Optimization.git
cd Traffic-Light-Optimization

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.venv\Scripts\activate
# Linux/Mac:
source .venv/bin/activate

# Install dependencies
pip install -r Requirements.txt

# Setup SUMO environment
# Windows: Set SUMO_HOME and add to PATH
setx SUMO_HOME "C:\Program Files (x86)\Eclipse\Sumo"
# Linux/Mac: Add to ~/.bashrc or ~/.zshrc
export SUMO_HOME="/usr/share/sumo"
export PATH="$SUMO_HOME/bin:$PATH"
```

### Training the Model

```bash
cd TCLS
python training_main.py
```

**Training Configuration** (`training_settings.ini`):
- Episodes: 100 (customizable)
- Duration: ~6 hours for 100 episodes
- GUI: Disabled by default (set `gui = True` to visualize)

### Testing the Model

```bash
cd TCLS
python testing_main.py
```

**Testing Configuration** (`testing_settings.ini`):
- Set `model_to_test = X` (model number to test)
- GUI enabled by default
- Single episode evaluation

---

## ğŸ“Š How It Works

### The AI Agent

**State Space (80 dimensions)**:
- 4 incoming directions (North, South, East, West)
- Each direction divided into 20 position cells
- Binary representation: 1 = vehicle present, 0 = empty
- Captures vehicle positions up to 750m from intersection

**Action Space (4 discrete actions)**:
- **Action 0**: North-South Green (straight/right turns)
- **Action 1**: North-South Left Green (left turns only)
- **Action 2**: East-West Green (straight/right turns)  
- **Action 3**: East-West Left Green (left turns only)

Each green phase lasts 10 seconds, with 4-second yellow transitions.

**Reward Function**:
```python
reward = previous_cumulative_waiting_time - current_cumulative_waiting_time
```
- Positive reward â†’ Reduced waiting time (good decision)
- Negative reward â†’ Increased waiting time (poor decision)

### Neural Network Architecture

```
Input Layer:    80 neurons (state representation)
Hidden Layer 1: 400 neurons (ReLU activation)
Hidden Layer 2: 400 neurons (ReLU activation)
Hidden Layer 3: 400 neurons (ReLU activation)
Hidden Layer 4: 400 neurons (ReLU activation)
Hidden Layer 5: 400 neurons (ReLU activation)
Output Layer:   4 neurons (Q-values for each action)
```

**Learning Algorithm**: Deep Q-Learning with Experience Replay

```
Q(s,a) = reward + Î³ Ã— max Q(s',a')
```

Where:
- `Î³ = 0.75` (discount factor)
- Experience replay buffer: 50,000 samples
- Batch size: 100
- Training epochs per episode: 800

---

## ğŸ“ Project Structure

```
Traffic-Light-Optimization/
â”œâ”€â”€ TCLS/                          # Main source code
â”‚   â”œâ”€â”€ training_main.py          # Training entry point
â”‚   â”œâ”€â”€ testing_main.py           # Testing entry point
â”‚   â”œâ”€â”€ model.py                  # Neural network definition
â”‚   â”œâ”€â”€ training_simulation.py    # Training simulation logic
â”‚   â”œâ”€â”€ testing_simulation.py     # Testing simulation logic
â”‚   â”œâ”€â”€ generator.py              # Traffic generation
â”‚   â”œâ”€â”€ memory.py                 # Experience replay buffer
â”‚   â”œâ”€â”€ utils.py                  # Helper functions
â”‚   â”œâ”€â”€ visualization.py          # Plotting utilities
â”‚   â”œâ”€â”€ training_settings.ini     # Training configuration
â”‚   â”œâ”€â”€ testing_settings.ini      # Testing configuration
â”‚   â”œâ”€â”€ intersection/             # SUMO simulation files
â”‚   â”‚   â”œâ”€â”€ environment.net.xml   # Road network definition
â”‚   â”‚   â”œâ”€â”€ episode_routes.rou.xml # Generated vehicle routes
â”‚   â”‚   â””â”€â”€ sumo_config.sumocfg   # SUMO configuration
â”‚   â””â”€â”€ models/                   # Saved trained models
â”‚       â””â”€â”€ model_X/
â”‚           â”œâ”€â”€ trained_model.h5  # Trained neural network
â”‚           â””â”€â”€ test/             # Test results
â”œâ”€â”€ .venv/                        # Virtual environment
â”œâ”€â”€ Requirements.txt              # Python dependencies
â”œâ”€â”€ LICENSE                       # MIT License
â”œâ”€â”€ README.md                     # This file
â””â”€â”€ DOCUMENTATION.md              # Detailed documentation
```

---

## ğŸ“ Technical Details

### Hyperparameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| Learning Rate | 0.001 | Adam optimizer learning rate |
| Discount Factor (Î³) | 0.75 | Future reward discount |
| Epsilon Decay | Linear (1.0 â†’ 0.0) | Exploration rate |
| Memory Size | 50,000 | Experience replay buffer |
| Batch Size | 100 | Training samples per batch |
| Training Epochs | 800 | Per episode training iterations |

### Performance Metrics

- **Cumulative Negative Reward**: Total waiting time penalty
- **Average Queue Length**: Mean number of waiting vehicles
- **Cumulative Delay**: Total seconds all vehicles waited

---

## ğŸ“ˆ Results

After 100 training episodes:
- âœ… Learned optimal traffic light control strategies
- âœ… Reduced average waiting time by 20-30% vs fixed timing
- âœ… Adapted to varying traffic patterns
- âœ… Made decisions in 30-50ms (real-time capable)

View results in `TCLS/models/model_X/test/`:
- `plot_reward.png` - Reward progression
- `plot_queue.png` - Queue length over time

---

## ğŸ› ï¸ Configuration

### Training Settings (`training_settings.ini`)

```ini
[simulation]
gui = False              # Enable/disable visualization
total_episodes = 100     # Number of training episodes
max_steps = 5400        # Steps per episode
n_cars_generated = 1000 # Traffic density

[model]
num_layers = 4          # Hidden layers (+ 1 = 5 total)
width_layers = 400      # Neurons per layer
batch_size = 100        # Training batch size
learning_rate = 0.001   # Adam optimizer rate
training_epochs = 800   # Training iterations/episode

[agent]
gamma = 0.75           # Discount factor
```

### Testing Settings (`testing_settings.ini`)

```ini
[simulation]
gui = True             # Enable visualization
model_to_test = 5     # Which model to test

[agent]
episode_seed = 10000  # Random seed for reproducibility
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **SUMO** - Simulation of Urban MObility
- **TensorFlow** - Machine Learning framework
- **Deep Q-Learning** - Based on DeepMind's DQN research

---

## ğŸ“š Further Reading

For detailed documentation, see [DOCUMENTATION.md](DOCUMENTATION.md)

For questions or issues, please open an issue on GitHub.



